{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"CMSIS-MLEK Reference Application Package This documentation explains the usage of the CMSIS-MLEK Reference Application Package for developing Machine Learning (ML) and Edge AI applications on Cortex-M processors with Ethos-U NPUs. Target Audience This user's guide assumes basic knowledge about Cortex-M software development and CMSIS-Toolbox workflows. It is written for embedded software developers who work with C/C++ compiler toolchains and develop machine learning applications for microcontroller devices with Cortex-M processors and Ethos-U NPUs. Package Overview The CMSIS-MLEK package provides ready-to-use templates and examples for machine learning applications. It includes three main template categories: Audio Reference Applications - Keyword spotting and audio processing examples Video Reference Applications - Object detection and video processing examples Generic Reference Applications - General-purpose inference runner for custom ML models (available from version 1.0+) Manual Chapters Overview explains the features and capabilities of the MLEK templates package. MLEK Reference Applications - Audio describes the available ML templates for audio use-cases. MLEK Reference Applications - Video describes the available ML templates for video and image use-cases. Target Configuration Reference Application Hardware gives step-by-step guidance for deploying the reference applications to a hardware platform in Visual Studion Code. Target Configuration Arm Virtual Hardware shows how to select a virtual target in Visual Studio code. Available Templates The package includes the following example templates: Template Description Key Features Audio/KWS Keyword spotting application Real-time audio processing, wake word detection Audio/User Algorithm Customizable audio processing template Template for implementing custom audio ML algorithms Video/Object Detection Object detection using camera input Real-time object detection, bounding box visualization Video/User Algorithm Customizable video processing template Template for implementing custom video ML algorithms Generic/Inference Runner General-purpose ML inference Support for custom TensorFlow Lite models Supported Platforms The templates support multiple Arm Cortex-M platforms: Corstone-300 (Cortex-M55 + Ethos-U55/U65) Corstone-310 (Cortex-M85 + Ethos-U55/U65) Corstone-315 (Cortex-M85 + Ethos-U65) Corstone-320 (Cortex-M85 + Ethos-U85) Each template can be built for both Arm Virtual Hardware (AVH) simulation and physical evaluation boards. Revision History Version Description 0.1.0 Initial draft release of CMSIS-MLEK templates package 0.5.0 First published release.","title":"Home"},{"location":"index.html#cmsis-mlek-reference-application-package","text":"This documentation explains the usage of the CMSIS-MLEK Reference Application Package for developing Machine Learning (ML) and Edge AI applications on Cortex-M processors with Ethos-U NPUs.","title":"CMSIS-MLEK Reference Application Package"},{"location":"index.html#target-audience","text":"This user's guide assumes basic knowledge about Cortex-M software development and CMSIS-Toolbox workflows. It is written for embedded software developers who work with C/C++ compiler toolchains and develop machine learning applications for microcontroller devices with Cortex-M processors and Ethos-U NPUs.","title":"Target Audience"},{"location":"index.html#package-overview","text":"The CMSIS-MLEK package provides ready-to-use templates and examples for machine learning applications. It includes three main template categories: Audio Reference Applications - Keyword spotting and audio processing examples Video Reference Applications - Object detection and video processing examples Generic Reference Applications - General-purpose inference runner for custom ML models (available from version 1.0+)","title":"Package Overview"},{"location":"index.html#manual-chapters","text":"Overview explains the features and capabilities of the MLEK templates package. MLEK Reference Applications - Audio describes the available ML templates for audio use-cases. MLEK Reference Applications - Video describes the available ML templates for video and image use-cases. Target Configuration Reference Application Hardware gives step-by-step guidance for deploying the reference applications to a hardware platform in Visual Studion Code. Target Configuration Arm Virtual Hardware shows how to select a virtual target in Visual Studio code.","title":"Manual Chapters"},{"location":"index.html#available-templates","text":"The package includes the following example templates: Template Description Key Features Audio/KWS Keyword spotting application Real-time audio processing, wake word detection Audio/User Algorithm Customizable audio processing template Template for implementing custom audio ML algorithms Video/Object Detection Object detection using camera input Real-time object detection, bounding box visualization Video/User Algorithm Customizable video processing template Template for implementing custom video ML algorithms Generic/Inference Runner General-purpose ML inference Support for custom TensorFlow Lite models","title":"Available Templates"},{"location":"index.html#supported-platforms","text":"The templates support multiple Arm Cortex-M platforms: Corstone-300 (Cortex-M55 + Ethos-U55/U65) Corstone-310 (Cortex-M85 + Ethos-U55/U65) Corstone-315 (Cortex-M85 + Ethos-U65) Corstone-320 (Cortex-M85 + Ethos-U85) Each template can be built for both Arm Virtual Hardware (AVH) simulation and physical evaluation boards.","title":"Supported Platforms"},{"location":"index.html#revision-history","text":"Version Description 0.1.0 Initial draft release of CMSIS-MLEK templates package 0.5.0 First published release.","title":"Revision History"},{"location":"overview.html","text":"Overview The Machine Learning Evaluation Kit (MLEK) pack contains CMSIS Reference Applications and templates for Edge AI development with embedded systems. These applications implement data preprocessing, memory management, and neural network inference pipelines that are optimized for Cortex-M and Ethos-U platforms. Key Features: Rapid Prototyping : Get working ML applications running quickly with minimal setup. Algorithm Development : Use example code as design patterns for custom ML algorithm implementation. Model Integration : Easily swap in custom TensorFlow Lite models with minimal code changes. Performance Validation : Test and optimize ML performance on target hardware or simulation. Hardware Evaluation : Compare performance across different Corstone platforms and configurations. The CMSIS-MLEK software pack is derived from the Arm\u00ae ML embedded evaluation kit and makes the examples easier to access. It also contains interfaces to physical hardware and simplifies porting to target hardware. It contains the following ML applications and uses currently Neural Network Models currently in TensorFlow Lite format. ML application Description Neural Network Model Keyword spotting (KWS) Recognize the presence of a key word in verbal speech MicroNet Object detection Detects and draws face bounding box in a given image Yolo Fastest Generic inference runner Code block allowing you to develop your own use case Your custom model Each ML reference application is a csolution project which supports deployment to physical hardware or Arm Virtual Hardware (AVH-FVP) for simulation. A board layer ( *.clayer.yml ) implements the drivers for the physical interfaces. The API interfaces required by the different applications is shown in the table below. Required API Interfaces Description Audio Processing CMSIS_VSTREAM_AUDIO_IN CMSIS-Driver vStream configured for Audio input. STDOUT Standard I/O for printf output. Video Processing CMSIS_VSTREAM_VIDEO_IN CMSIS-Driver vStream configured for Video input. CMSIS_VSTREAM_VIDEO_OUT CMSIS-Driver vStream configured for Video output. STDOUT Standard I/O for printf output. Generic Inference Runner STDOUT Standard I/O for printf output. Platform Support The templates support via target names multiple Arm Cortex-M IP Subsystems . These target names support execution on AVH FVP simulation models which is useful during software development or with Contiguous Integration (CI) testing using GitHub actions. Target Name IP Subsystem Description AVH-SSE-300 Corstone-300 Cortex-M55 optional with Ethos-U55 or Ethos-U65 AVH-SSE-310 Corstone-310 Cortex-M85 optional with Ethos-U55 AVH-SSE-315 Corstone-315 Cortex-M85 optional with Ethos-U65 AVH-SSE-320 Corstone-320 Cortex-M85 optional with Ethos-U85 Adding a postfix to the target name in the *.csolution.yml project file configures the neural network inference pipeline for Ethos-U. Without this prefix only the Cortex-M system is used as shown in the diagram below. Such a postfix can also be used for target names that deploy to physical hardware. Postfix Description none Cortex-M system only, no Ethos-U NPU -U55 Cortex-M system + Ethos-U55 NPU -U65 Cortex-M system + Ethos-U65 NPU -U85 Cortex-M system + Ethos-U85 NPU","title":"Overview"},{"location":"overview.html#overview","text":"The Machine Learning Evaluation Kit (MLEK) pack contains CMSIS Reference Applications and templates for Edge AI development with embedded systems. These applications implement data preprocessing, memory management, and neural network inference pipelines that are optimized for Cortex-M and Ethos-U platforms. Key Features: Rapid Prototyping : Get working ML applications running quickly with minimal setup. Algorithm Development : Use example code as design patterns for custom ML algorithm implementation. Model Integration : Easily swap in custom TensorFlow Lite models with minimal code changes. Performance Validation : Test and optimize ML performance on target hardware or simulation. Hardware Evaluation : Compare performance across different Corstone platforms and configurations. The CMSIS-MLEK software pack is derived from the Arm\u00ae ML embedded evaluation kit and makes the examples easier to access. It also contains interfaces to physical hardware and simplifies porting to target hardware. It contains the following ML applications and uses currently Neural Network Models currently in TensorFlow Lite format. ML application Description Neural Network Model Keyword spotting (KWS) Recognize the presence of a key word in verbal speech MicroNet Object detection Detects and draws face bounding box in a given image Yolo Fastest Generic inference runner Code block allowing you to develop your own use case Your custom model Each ML reference application is a csolution project which supports deployment to physical hardware or Arm Virtual Hardware (AVH-FVP) for simulation. A board layer ( *.clayer.yml ) implements the drivers for the physical interfaces. The API interfaces required by the different applications is shown in the table below. Required API Interfaces Description Audio Processing CMSIS_VSTREAM_AUDIO_IN CMSIS-Driver vStream configured for Audio input. STDOUT Standard I/O for printf output. Video Processing CMSIS_VSTREAM_VIDEO_IN CMSIS-Driver vStream configured for Video input. CMSIS_VSTREAM_VIDEO_OUT CMSIS-Driver vStream configured for Video output. STDOUT Standard I/O for printf output. Generic Inference Runner STDOUT Standard I/O for printf output.","title":"Overview"},{"location":"overview.html#platform-support","text":"The templates support via target names multiple Arm Cortex-M IP Subsystems . These target names support execution on AVH FVP simulation models which is useful during software development or with Contiguous Integration (CI) testing using GitHub actions. Target Name IP Subsystem Description AVH-SSE-300 Corstone-300 Cortex-M55 optional with Ethos-U55 or Ethos-U65 AVH-SSE-310 Corstone-310 Cortex-M85 optional with Ethos-U55 AVH-SSE-315 Corstone-315 Cortex-M85 optional with Ethos-U65 AVH-SSE-320 Corstone-320 Cortex-M85 optional with Ethos-U85 Adding a postfix to the target name in the *.csolution.yml project file configures the neural network inference pipeline for Ethos-U. Without this prefix only the Cortex-M system is used as shown in the diagram below. Such a postfix can also be used for target names that deploy to physical hardware. Postfix Description none Cortex-M system only, no Ethos-U NPU -U55 Cortex-M system + Ethos-U55 NPU -U65 Cortex-M system + Ethos-U65 NPU -U85 Cortex-M system + Ethos-U85 NPU","title":"Platform Support"},{"location":"target_configuration_avh.html","text":"Target Configuration Arm Virtual Hardware Supported Platforms The MLEK templates support multiple virtual Arm Cortex-M platforms with Ethos-U NPU acceleration: Platform Processor NPU Options Reference Application Support Corstone-300 Cortex-M55 Ethos-U55, Ethos-U65, No NPU All Corstone-310 Cortex-M85 Ethos-U55, Ethos-U65, No NPU All Corstone-315 Cortex-M85 Ethos-U65 All Corstone-320 Cortex-M85 Ethos-U85 All These targets are preconfigured in the solution and allow to run the applications without hardware. Using VS Code Open the Manage Solution dialog from the CMSIS Solution Extension window (see user guide for details). You can select an arbitrary FVP platform. Refer to the the above table, to find out which Cortex-M CPU core and which Ethos-U NPU will be simulated. Select the Build Types *-Data_Array to create an application image that includes test data for the model. A run configuration will automatically be created, so you can start the model and the application in the internal VSCode terminal, with the Run feature.","title":"Target Configuration Arm Virtual Hardware"},{"location":"target_configuration_avh.html#target-configuration-arm-virtual-hardware","text":"","title":"Target Configuration Arm Virtual Hardware"},{"location":"target_configuration_avh.html#supported-platforms","text":"The MLEK templates support multiple virtual Arm Cortex-M platforms with Ethos-U NPU acceleration: Platform Processor NPU Options Reference Application Support Corstone-300 Cortex-M55 Ethos-U55, Ethos-U65, No NPU All Corstone-310 Cortex-M85 Ethos-U55, Ethos-U65, No NPU All Corstone-315 Cortex-M85 Ethos-U65 All Corstone-320 Cortex-M85 Ethos-U85 All These targets are preconfigured in the solution and allow to run the applications without hardware.","title":"Supported Platforms"},{"location":"target_configuration_avh.html#using-vs-code","text":"Open the Manage Solution dialog from the CMSIS Solution Extension window (see user guide for details). You can select an arbitrary FVP platform. Refer to the the above table, to find out which Cortex-M CPU core and which Ethos-U NPU will be simulated. Select the Build Types *-Data_Array to create an application image that includes test data for the model. A run configuration will automatically be created, so you can start the model and the application in the internal VSCode terminal, with the Run feature.","title":"Using VS Code"},{"location":"target_configuration_refapp.html","text":"Target Configuration Reference Application Hardware Using VS Code This section explains how to use MLEK reference applications with the Arm CMSIS Solution extension for VS Code. Install Required Packs Install the CMSIS-MLEK pack and any required board support packs: cpackget add ARM::cmsis-mlek Create New Solution Open VS Code and use the Create a new solution dialog Select one of the MLEK reference applications: MLEK Keyword Spotting and Audio User Algorithm Template MLEK Object Detection and Video User Algorithm Template MLEK Generic Inference Runnner . (available in pack version 1.0+) Configure the target platform and toolchain: Build and Run Refer to the Arm CMSIS CSolution extentions documentation on how to use the CMSIS View. It offers convenient and pre-configured access to build and debug features. Enable Ethos-U NPU support for your target To enable NPU support for custom hardware: Open the *.csolution.yml file and look for the first target-type : target-types: - type: MyCustomBoard device: STM32U585AIIx # Your target MCU variables: ... To the \"type\" entry add a suffix that identifies NPU model and MAC configuration. e.g.: - type: MyCustomBoard-U55-128 # or - type: MyCustomBoard-U85-512 This will configure the project to include the correct drivers and models for the NPU selected. Reference Applications Audio Template: Keyword Spotting The KWS template demonstrates real-time wake word detection: Key Features: Real-time audio preprocessing (MFCC feature extraction) Optimized neural network inference using CMSIS-NN Configurable wake word models Performance profiling and benchmarking Getting Started: Build and run the template on your target platform Speak the wake word (e.g. \"Yes\" or \"Up\") Observe detection results via UART output or LEDs Replace the model with your custom wake word model Customization Points: kws/src/kws_model.cpp : Replace with your TensorFlow Lite model kws/src/audio_preprocessing.cpp : Modify audio preprocessing pipeline kws/config/kws_config.h : Adjust detection thresholds and parameters Video Template: Object Detection The object detection template provides real-time computer vision: Key Features: Camera input processing and frame buffering Object detection using MobileNet-based models Bounding box visualization Multi-object detection and classification Getting Started: Build and run the template with camera input Point camera at objects for detection View detection results on display or via debug output Integrate your custom object detection model Customization Points: object-detection/src/detection_model.cpp : Replace with your model object-detection/src/image_preprocessing.cpp : Modify image preprocessing object-detection/config/detection_config.h : Adjust detection parameters Generic Template: Inference Runner (available in pack version 1.0+) The generic inference template provides maximum flexibility: Key Features: Framework for any TensorFlow Lite model Configurable input/output tensors Performance benchmarking utilities Extensible architecture for custom applications Getting Started: Replace the example model with your TensorFlow Lite model Configure input/output tensor specifications Implement custom preprocessing/postprocessing Build and test your custom ML application Customization Points: inference_runner/Model/ : Replace with your TensorFlow Lite model files inference_runner/src/inference_runner.cpp : Modify inference pipeline inference_runner/config/ : Adjust model and application configuration Performance Optimization All MLEK templates include built-in performance optimization features: CMSIS-NN Integration : Optimized neural network kernels for Cortex-M Ethos-U Acceleration : NPU acceleration for supported layers Memory Optimization : Efficient memory management and tensor allocation Profiling Tools : Built-in timing and resource usage measurements Next Steps After successfully running an MLEK template: Model Integration : Replace the example model with your trained TensorFlow Lite model Application Customization : Modify the application logic for your specific use case Performance Tuning : Optimize for your target constraints (memory, power, latency) Hardware Deployment : Test on your target hardware platform Production Deployment : Integrate into your final product design For detailed examples and additional resources, refer to the individual template README files and the MLEK documentation.","title":"Target Configuration Hardware Boards"},{"location":"target_configuration_refapp.html#target-configuration-reference-application-hardware","text":"","title":"Target Configuration Reference Application Hardware"},{"location":"target_configuration_refapp.html#using-vs-code","text":"This section explains how to use MLEK reference applications with the Arm CMSIS Solution extension for VS Code.","title":"Using VS Code"},{"location":"target_configuration_refapp.html#install-required-packs","text":"Install the CMSIS-MLEK pack and any required board support packs: cpackget add ARM::cmsis-mlek","title":"Install Required Packs"},{"location":"target_configuration_refapp.html#create-new-solution","text":"Open VS Code and use the Create a new solution dialog Select one of the MLEK reference applications: MLEK Keyword Spotting and Audio User Algorithm Template MLEK Object Detection and Video User Algorithm Template MLEK Generic Inference Runnner . (available in pack version 1.0+) Configure the target platform and toolchain:","title":"Create New Solution"},{"location":"target_configuration_refapp.html#build-and-run","text":"Refer to the Arm CMSIS CSolution extentions documentation on how to use the CMSIS View. It offers convenient and pre-configured access to build and debug features.","title":"Build and Run"},{"location":"target_configuration_refapp.html#enable-ethos-u-npu-support-for-your-target","text":"To enable NPU support for custom hardware: Open the *.csolution.yml file and look for the first target-type : target-types: - type: MyCustomBoard device: STM32U585AIIx # Your target MCU variables: ... To the \"type\" entry add a suffix that identifies NPU model and MAC configuration. e.g.: - type: MyCustomBoard-U55-128 # or - type: MyCustomBoard-U85-512 This will configure the project to include the correct drivers and models for the NPU selected.","title":"Enable Ethos-U NPU support for your target"},{"location":"target_configuration_refapp.html#reference-applications","text":"","title":"Reference Applications"},{"location":"target_configuration_refapp.html#audio-template-keyword-spotting","text":"The KWS template demonstrates real-time wake word detection: Key Features: Real-time audio preprocessing (MFCC feature extraction) Optimized neural network inference using CMSIS-NN Configurable wake word models Performance profiling and benchmarking Getting Started: Build and run the template on your target platform Speak the wake word (e.g. \"Yes\" or \"Up\") Observe detection results via UART output or LEDs Replace the model with your custom wake word model Customization Points: kws/src/kws_model.cpp : Replace with your TensorFlow Lite model kws/src/audio_preprocessing.cpp : Modify audio preprocessing pipeline kws/config/kws_config.h : Adjust detection thresholds and parameters","title":"Audio Template: Keyword Spotting"},{"location":"target_configuration_refapp.html#video-template-object-detection","text":"The object detection template provides real-time computer vision: Key Features: Camera input processing and frame buffering Object detection using MobileNet-based models Bounding box visualization Multi-object detection and classification Getting Started: Build and run the template with camera input Point camera at objects for detection View detection results on display or via debug output Integrate your custom object detection model Customization Points: object-detection/src/detection_model.cpp : Replace with your model object-detection/src/image_preprocessing.cpp : Modify image preprocessing object-detection/config/detection_config.h : Adjust detection parameters","title":"Video Template: Object Detection"},{"location":"target_configuration_refapp.html#generic-template-inference-runner","text":"(available in pack version 1.0+) The generic inference template provides maximum flexibility: Key Features: Framework for any TensorFlow Lite model Configurable input/output tensors Performance benchmarking utilities Extensible architecture for custom applications Getting Started: Replace the example model with your TensorFlow Lite model Configure input/output tensor specifications Implement custom preprocessing/postprocessing Build and test your custom ML application Customization Points: inference_runner/Model/ : Replace with your TensorFlow Lite model files inference_runner/src/inference_runner.cpp : Modify inference pipeline inference_runner/config/ : Adjust model and application configuration","title":"Generic Template: Inference Runner"},{"location":"target_configuration_refapp.html#performance-optimization","text":"All MLEK templates include built-in performance optimization features: CMSIS-NN Integration : Optimized neural network kernels for Cortex-M Ethos-U Acceleration : NPU acceleration for supported layers Memory Optimization : Efficient memory management and tensor allocation Profiling Tools : Built-in timing and resource usage measurements","title":"Performance Optimization"},{"location":"target_configuration_refapp.html#next-steps","text":"After successfully running an MLEK template: Model Integration : Replace the example model with your trained TensorFlow Lite model Application Customization : Modify the application logic for your specific use case Performance Tuning : Optimize for your target constraints (memory, power, latency) Hardware Deployment : Test on your target hardware platform Production Deployment : Integrate into your final product design For detailed examples and additional resources, refer to the individual template README files and the MLEK documentation.","title":"Next Steps"},{"location":"templates_audio.html","text":"MLEK Reference Applications - Audio Audio Processing Reference Applications These reference applications focus on real-time audio processing: Keyword Spotting (KWS) : Demonstrates wake word detection and voice command recognition Audio User Algorithm Template : Provides a foundation for custom audio ML processing applications Required API Interfaces For hardware deployment, the Board-Layer should provide the following API interfaces: Required API Interface Description CMSIS_VIO Virtual I/O interface for LEDs, buttons, and basic I/O CMSIS_VSTREAM_AUDIO_IN Virtual Audio Input / Audio Interface STDOUT, STDERR Standard output for printf debugging and logging These interfaces ideally are supplied by the vendor of your evaluation board. For custom hardware, details on the implementation are found in the CMSIS-Driver Manual Keyword Spotting Application Keyword spotting (KWS) is the process of detecting predefined words or phrases from a continuous audio stream. In the CMSIS-MLEK templates this allows an embedded device to listen for a \"wake word\" before executing further commands. The audio template provides an end-to-end implementation using TensorFlow Lite Micro and CMSIS-NN for optimized inference on Cortex-M processors. How KWS Works Incoming audio is captured from a microphone or played back from a test sample. The audio stream is converted into Mel-frequency cepstral coefficients (MFCC) features. A neural network classifies the MFCC features to determine which keyword, if any, was spoken. Detection results are reported via UART or LED indicators. The template can detect up to twelve keywords. A sample audio file containing the word \"down\" is provided for testing. This capture shows serial output from a hardware target, while the application detects the keyword \"yes\" on a microphone stream. Build Types The KWS example defines four build types that control debug information and the audio source: Build Type Description Debug-Live_Stream Uses live microphone input with debug information enabled. Release-Live_Stream Live microphone input with optimizations for performance. Debug-Data_Array Processes a built-in audio array for regression testing with debug information. Release-Data_Array Processes the audio array with release optimizations. Use the Debug build types during development and the Release build types for performance measurements. Switch between Live_Stream and Data_Array depending on whether you want real-time audio or a fixed sample. On Arm Virtual Hardware Targets, the Live_Stream is utilizing the VSI interface Audio User Algorithm Template Todo. Working with MLEK Templates See Target Configuration chapters on how to deploy the reference applications to a specific hardware or simulation target.","title":"Audio Template Applications"},{"location":"templates_audio.html#mlek-reference-applications-audio","text":"","title":"MLEK Reference Applications - Audio"},{"location":"templates_audio.html#audio-processing-reference-applications","text":"These reference applications focus on real-time audio processing: Keyword Spotting (KWS) : Demonstrates wake word detection and voice command recognition Audio User Algorithm Template : Provides a foundation for custom audio ML processing applications","title":"Audio Processing Reference Applications"},{"location":"templates_audio.html#required-api-interfaces","text":"For hardware deployment, the Board-Layer should provide the following API interfaces: Required API Interface Description CMSIS_VIO Virtual I/O interface for LEDs, buttons, and basic I/O CMSIS_VSTREAM_AUDIO_IN Virtual Audio Input / Audio Interface STDOUT, STDERR Standard output for printf debugging and logging These interfaces ideally are supplied by the vendor of your evaluation board. For custom hardware, details on the implementation are found in the CMSIS-Driver Manual","title":"Required API Interfaces"},{"location":"templates_audio.html#keyword-spotting-application","text":"Keyword spotting (KWS) is the process of detecting predefined words or phrases from a continuous audio stream. In the CMSIS-MLEK templates this allows an embedded device to listen for a \"wake word\" before executing further commands. The audio template provides an end-to-end implementation using TensorFlow Lite Micro and CMSIS-NN for optimized inference on Cortex-M processors.","title":"Keyword Spotting Application"},{"location":"templates_audio.html#how-kws-works","text":"Incoming audio is captured from a microphone or played back from a test sample. The audio stream is converted into Mel-frequency cepstral coefficients (MFCC) features. A neural network classifies the MFCC features to determine which keyword, if any, was spoken. Detection results are reported via UART or LED indicators. The template can detect up to twelve keywords. A sample audio file containing the word \"down\" is provided for testing. This capture shows serial output from a hardware target, while the application detects the keyword \"yes\" on a microphone stream.","title":"How KWS Works"},{"location":"templates_audio.html#build-types","text":"The KWS example defines four build types that control debug information and the audio source: Build Type Description Debug-Live_Stream Uses live microphone input with debug information enabled. Release-Live_Stream Live microphone input with optimizations for performance. Debug-Data_Array Processes a built-in audio array for regression testing with debug information. Release-Data_Array Processes the audio array with release optimizations. Use the Debug build types during development and the Release build types for performance measurements. Switch between Live_Stream and Data_Array depending on whether you want real-time audio or a fixed sample. On Arm Virtual Hardware Targets, the Live_Stream is utilizing the VSI interface","title":"Build Types"},{"location":"templates_audio.html#audio-user-algorithm-template","text":"Todo.","title":"Audio User Algorithm Template"},{"location":"templates_audio.html#working-with-mlek-templates","text":"See Target Configuration chapters on how to deploy the reference applications to a specific hardware or simulation target.","title":"Working with MLEK Templates"},{"location":"templates_video.html","text":"MLEK Reference Application Templates Video Processing Templates These templates focus on real-time audio processing: Keyword Spotting (KWS) : Demonstrates wake word detection and voice command recognition Audio User Algorithm Template : Provides a foundation for custom audio ML processing applications Required API Interfaces For hardware deployment, the Board-Layer should provide the following API interfaces: Required API Interface Description CMSIS_VIO Virtual I/O interface for LEDs, buttons, and basic I/O CMSIS_VSTREAM_VIDEO_IN Virtual Video Input / Camera Interface CMSIS_VSTREAM_VIDEO_OUT Virtual Video Output / Display STDOUT, STDERR Standard output for printf debugging and logging These interfaces ideally are supplied by the vendor of your evaluation board. For custom hardware, details on the implementation are found in the CMSIS-Driver Manual Object Detection Application This example uses a neural network model that specialises in detecting human faces in images. The input size for these images is 192x192 (monochrome) and the smallest face that can be detected is of size 20x20. The output of the application will be co-ordinates for rectangular bounding boxes for each detection. Build Types The Object Detection example defines four build types that control debug information and the video source: Build Type Description Debug-Live_Stream Capture frames from a camera in real time. Debug information enabled. Release-Live_Stream Capture frames from a camera in real time. With optimizations for performance. Debug-Data_Array Built-in image data for regression testing with debug information. Release-Data_Array Built-in image data with release optimizations. Use the Debug build types during development and the Release build types for performance measurements. Switch between Live_Stream and Data_Array depending on whether you want real-time video or a fixed sample. On Arm Virtual Hardware Targets, the Live_Stream is utilizing the VSI interface Video User Algorithm Template Todo Working with MLEK Templates See Target Configuration chapters on how to deploy the reference applications to a specific hardware or simulation target.","title":"Video Template Applications"},{"location":"templates_video.html#mlek-reference-application-templates","text":"","title":"MLEK Reference Application Templates"},{"location":"templates_video.html#video-processing-templates","text":"These templates focus on real-time audio processing: Keyword Spotting (KWS) : Demonstrates wake word detection and voice command recognition Audio User Algorithm Template : Provides a foundation for custom audio ML processing applications","title":"Video Processing Templates"},{"location":"templates_video.html#required-api-interfaces","text":"For hardware deployment, the Board-Layer should provide the following API interfaces: Required API Interface Description CMSIS_VIO Virtual I/O interface for LEDs, buttons, and basic I/O CMSIS_VSTREAM_VIDEO_IN Virtual Video Input / Camera Interface CMSIS_VSTREAM_VIDEO_OUT Virtual Video Output / Display STDOUT, STDERR Standard output for printf debugging and logging These interfaces ideally are supplied by the vendor of your evaluation board. For custom hardware, details on the implementation are found in the CMSIS-Driver Manual","title":"Required API Interfaces"},{"location":"templates_video.html#object-detection-application","text":"This example uses a neural network model that specialises in detecting human faces in images. The input size for these images is 192x192 (monochrome) and the smallest face that can be detected is of size 20x20. The output of the application will be co-ordinates for rectangular bounding boxes for each detection.","title":"Object Detection Application"},{"location":"templates_video.html#build-types","text":"The Object Detection example defines four build types that control debug information and the video source: Build Type Description Debug-Live_Stream Capture frames from a camera in real time. Debug information enabled. Release-Live_Stream Capture frames from a camera in real time. With optimizations for performance. Debug-Data_Array Built-in image data for regression testing with debug information. Release-Data_Array Built-in image data with release optimizations. Use the Debug build types during development and the Release build types for performance measurements. Switch between Live_Stream and Data_Array depending on whether you want real-time video or a fixed sample. On Arm Virtual Hardware Targets, the Live_Stream is utilizing the VSI interface","title":"Build Types"},{"location":"templates_video.html#video-user-algorithm-template","text":"Todo","title":"Video User Algorithm Template"},{"location":"templates_video.html#working-with-mlek-templates","text":"See Target Configuration chapters on how to deploy the reference applications to a specific hardware or simulation target.","title":"Working with MLEK Templates"}]}